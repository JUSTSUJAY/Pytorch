{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12978a89",
   "metadata": {},
   "source": [
    "Data does not always come in its final processed form that is required for training machine learning algorithms. We use transforms to perform some manipulation of the data and make it suitable for training.\n",
    "\n",
    "All TorchVision datasets have two parameters -transform to modify the features and target_transform to modify the labels - that accept callables containing the transformation logic. The torchvision.transforms module offers several commonly-used transforms out of the box.\n",
    "\n",
    "The FashionMNIST features are in PIL Image format, and the labels are integers. For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors. To make these transformations, we use ToTensor and Lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03812b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62f8d351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp/ipykernel_12928/3196161897.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.zeros(10,dtype = torch.float).scatter_(0,torch.tensor(y),value = 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor(4)\n",
    "x = torch.zeros(10,dtype = torch.float).scatter_(0,torch.tensor(y),value = 1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7287ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.FashionMNIST(\n",
    "    root = '../data',\n",
    "    train = True, \n",
    "    download = False,\n",
    "    transform = ToTensor(), # to normalized tensors\n",
    "    target_transform = Lambda(lambda y: torch.zeros(10,dtype = torch.float).scatter_(0,torch.tensor(y),value = 1)) # to one hot encode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "488ed5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd708638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we use ToTensor() it converts it into a tensor and also normalizes it in 0-1 range\n",
    "# when we use Lambda for target transform then what do we do\n",
    "# first we said torch.zeros(10,dtype = torch.float) ----- [0.0,0.0......10 times]\n",
    "# scatter_(0,torch.tensor(y),value = 1) will replace 1 at index of y else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af64ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
