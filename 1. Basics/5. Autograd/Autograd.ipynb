{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee05ea59",
   "metadata": {},
   "source": [
    "- pytorch has a built in differentiation engine which is used to calculate the gradients and all backpropagation depends on it\n",
    "- it supports automatic computation of gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee485b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# input tensor\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "w = torch.rand(5,3,requires_grad = True)\n",
    "b = torch.rand(3,requires_grad = True)\n",
    "z = torch.matmul(x,w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152a5e41",
   "metadata": {},
   "source": [
    "In this network, w and b are parameters, which we need to optimize. Thus, **we need to be able to compute the gradients of loss function with respect to those variables**. In order to do that, we set the requires_grad property of those tensors.\n",
    "\n",
    "You can set the value of `requires_grad` when creating a tensor, or later by using `x.requires_grad_(True)` method.\n",
    "\n",
    "- A function that we apply to tensors to construct computational graph is in fact an object of class `Function`.\n",
    "This object of class `Function` knows \n",
    "    - how to compute the function in forward direction\n",
    "    - how to calculate it's derivative in backward direction\n",
    "    - reference to backward propagation is stored in `grad_fn` property of a tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e72b2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x000001EE67D4CE20>\n",
      "<BinaryCrossEntropyWithLogitsBackward0 object at 0x000001EE67D52850>\n"
     ]
    }
   ],
   "source": [
    "print(z.grad_fn)\n",
    "print(loss.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e51b02e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3039, 0.3217, 0.2949],\n",
      "        [0.3039, 0.3217, 0.2949],\n",
      "        [0.3039, 0.3217, 0.2949],\n",
      "        [0.3039, 0.3217, 0.2949],\n",
      "        [0.3039, 0.3217, 0.2949]])\n",
      "tensor([0.3039, 0.3217, 0.2949])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a2975a",
   "metadata": {},
   "source": [
    "To Optimize this weights and biases we need to take gradient or derivate of losses w.r.t to this variables - keeping the values of output and input constant\n",
    "\n",
    "this gradients are calculated after calling loss.backward() function <br>\n",
    "and then we call w.grad and b.grad to know their values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ddbfdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c51f46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2863, 0.2945, 0.3069],\n",
      "        [0.2863, 0.2945, 0.3069],\n",
      "        [0.2863, 0.2945, 0.3069],\n",
      "        [0.2863, 0.2945, 0.3069],\n",
      "        [0.2863, 0.2945, 0.3069]])\n",
      "tensor([0.2863, 0.2945, 0.3069])\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2bf513",
   "metadata": {},
   "source": [
    "we can use `.grad` property on only the leaf nodes such as w and b which we wanted to optimize and for them we had `requires_grad = True` and for all the other notes `.grad` is not available.\n",
    "\n",
    "As we can see we got the same values for all of them because the `loss.backward()` was performed only once in our case and if we want to use it for multiple times we need to set `loss.backward(retain_graph = True)`.\n",
    "\n",
    "## Disabling gradient Tracking\n",
    "\n",
    "IN case of testing our model we do not need to modify our weights rather keep them as it is, hence for those test inputs we don't need to keep our parameter of `requires_grad = True` \n",
    "\n",
    "In such cases we only need forward pass. we can then stop this computation by adding `torch.no_grad()` meaning don't change the gradients or weights, keep them as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88192d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x,w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x,w) + b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbb7bd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# another way to use this is to use `detach()` method\n",
    "\n",
    "z = (torch.matmul(x,w)+b).detach()\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c9e31e",
   "metadata": {},
   "source": [
    "## Process of computational graphs\n",
    "\n",
    "- autograd keeps the record of both **data(tensors) and all the executed operations(along with the new tensors)**\n",
    "- they are stored in DAG **Directed Acyclic Groups**\n",
    "- IN DAGs leaves are the input tensors and roots are the output tensors\n",
    "- by tracing from leaves to nodes we are automatically computing the gradients using chain rule\n",
    "\n",
    "### In a forward pass, autograd does two things simultaneously:\n",
    "\n",
    "- run the requested operation to compute a resulting tensor\n",
    "\n",
    "- maintain the operationâ€™s gradient function in the DAG.\n",
    "\n",
    "### BAckward pass\n",
    "    1 when we use `.backward` it is called DAG root.autograd`\n",
    "    - computes and gradients from each `.grad` attribute\n",
    "    \n",
    "**Note** - DAGs are dynamic in PyTorch An important thing to note is that the `graph is recreated from scratch; after each .backward() call`, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfc1782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
