{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f5e11e4",
   "metadata": {},
   "source": [
    "While working with data pytorch has two primitives\n",
    "1. torch.utils.data.DataLoader(Wraps an iterable around the dataset)\n",
    "2. torch.utils.data.Dataset(Stores samples and their labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99378d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8cfb20",
   "metadata": {},
   "source": [
    "PyTorch offers domain-specific libraries such as **TorchText, TorchVision, and TorchAudio**, all of which include datasets. For this tutorial, we will be using a TorchVision dataset.\n",
    "\n",
    "`torchvision.datasets` module contains Dataset Object for many real world vision data, we will be using fashion MNIST\n",
    "- Every torchvision dataset has two arguments **transform** and **target_transform** to modify samples and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4dec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20411c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = datasets.FashionMNIST(root = 'data',train = False, download = False, transform = ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c66f2a",
   "metadata": {},
   "source": [
    "training and testing_data or instances of Dataset class which will be passed to DataLoader to wrap an iterable around our dataset\n",
    "\n",
    "why do we wrap an iterable?\n",
    "  - it supports automatic batching, sampling, shuffling, multiprocess data loading\n",
    " \n",
    "here we will keep the batch size of 64, so each element of Dataloader iterable will return a batch of 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d6d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training dataset 60000\n",
      "Number of training batches 938\n",
      "\n",
      "From this we can conclude that when we say a dataloader,it has the dimension as follows\n",
      "dataloader has 938 batches, \n",
      "length of testing dataset 10000\n",
      "Number of testing batches 157\n",
      "\n",
      "train dataloader has 938 batches\n",
      "Each batch has 64 images\n",
      "Each image has dimension as 28*28\n",
      "Each batch has torch.Size([64]) labels\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_data,batch_size = batch_size)\n",
    "test_dataloader = DataLoader(testing_data, batch_size = batch_size)\n",
    "\n",
    "# lets take a moment to understand how our data is divided after forming the batches\n",
    "# firstly we had 60k images, each of size 28*28\n",
    "# then we wrapped it around the dataloader object in 'train_dataloader' and divided it by the batch of 64\n",
    "# so we now have 60000/64 ~ 938 batches, each containing 64 images each of size 28*28\n",
    "# so for the first batch we have 64 images and 1 image of size 28*28\n",
    "# dimensions being -> (64,1,28,28)\n",
    "\n",
    "print('length of training dataset', len(train_dataloader.dataset))\n",
    "print('Number of training batches',int(len(train_dataloader)))\n",
    "print()\n",
    "print('From this we can conclude that when we say a dataloader,it has the dimension as follows')\n",
    "print(f'dataloader has {len(train_dataloader)} batches, ')\n",
    "\n",
    "print('length of testing dataset', len(test_dataloader.dataset))\n",
    "print('Number of testing batches',int(len(test_dataloader)))\n",
    "\n",
    "print()\n",
    "for X,y in train_dataloader:\n",
    "    print(f'train dataloader has {len(train_dataloader)} batches')\n",
    "    print(f'Each batch has {X.shape[0]} images')\n",
    "    print(f'Each image has dimension as {X.shape[2]}*{X.shape[3]}')\n",
    "    print(f'Each batch has {y.shape} labels')\n",
    "    print(y[0].item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c022d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# create a model\n",
    "device = ('cpu')\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "984d2bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1298829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and an otpimizer \n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acafaac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader,model,loss_fun,optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    model.train()\n",
    "    for batch,(x,y) in enumerate(dataloader):\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        num_batches = len(x)\n",
    "#         predict and calc error\n",
    "        pred = model(x)\n",
    "        loss = loss_fun(pred,y)\n",
    "        \n",
    "#         Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if (batch%100 == 0) | (batch == len(dataloader)):\n",
    "            loss, current = loss.item(),(batch+1)*num_batches\n",
    "            print(f\"Loss : {loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af9a3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader,model,loss_fun):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "#     torch.no_grad() is passed so that it does not calculate any gradients during testset and works with previously calculated ones\n",
    "    with torch.no_grad():\n",
    "        for x,y in dataloader:\n",
    "            x,y = x.to(device),y.to(device)\n",
    "            pred = model(x)\n",
    "            test_loss += loss_fun(pred,y).item()\n",
    "            correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error \\n Accuracy : {(100*correct):>0.1f}% \\n Avg_loss: {test_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd182819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no :  1\n",
      "Loss : 2.308845 [   64/60000]\n",
      "Loss : 2.298676 [ 6464/60000]\n",
      "Loss : 2.279666 [12864/60000]\n",
      "Loss : 2.270586 [19264/60000]\n",
      "Loss : 2.262078 [25664/60000]\n",
      "Loss : 2.226506 [32064/60000]\n",
      "Loss : 2.245336 [38464/60000]\n",
      "Loss : 2.208933 [44864/60000]\n",
      "Loss : 2.208906 [51264/60000]\n",
      "Loss : 2.179075 [57664/60000]\n",
      "Test Error \n",
      " Accuracy : 31.1% \n",
      " Avg_loss: 2.178170\n",
      "\n",
      "epoch no :  2\n",
      "Loss : 2.188558 [   64/60000]\n",
      "Loss : 2.188727 [ 6464/60000]\n",
      "Loss : 2.133457 [12864/60000]\n",
      "Loss : 2.147375 [19264/60000]\n",
      "Loss : 2.113538 [25664/60000]\n",
      "Loss : 2.046915 [32064/60000]\n",
      "Loss : 2.086796 [38464/60000]\n",
      "Loss : 2.008084 [44864/60000]\n",
      "Loss : 2.015861 [51264/60000]\n",
      "Loss : 1.959991 [57664/60000]\n",
      "Test Error \n",
      " Accuracy : 55.7% \n",
      " Avg_loss: 1.955016\n",
      "\n",
      "epoch no :  3\n",
      "Loss : 1.980700 [   64/60000]\n",
      "Loss : 1.969141 [ 6464/60000]\n",
      "Loss : 1.851595 [12864/60000]\n",
      "Loss : 1.890064 [19264/60000]\n",
      "Loss : 1.804870 [25664/60000]\n",
      "Loss : 1.733507 [32064/60000]\n",
      "Loss : 1.767676 [38464/60000]\n",
      "Loss : 1.656680 [44864/60000]\n",
      "Loss : 1.683763 [51264/60000]\n",
      "Loss : 1.588911 [57664/60000]\n",
      "Test Error \n",
      " Accuracy : 60.6% \n",
      " Avg_loss: 1.595506\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print('epoch no : ',epoch+1)\n",
    "    train(train_dataloader,model, loss_fun, optimizer)\n",
    "    test(test_dataloader,model, loss_fun)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6daea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ef01ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466c8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
